# å·¥ä½œå•ä½ ï¼š æ­å·ç”µå­ç§‘æŠ€å¤§å­¦
# å§“å ï¼š ç½—ä¼šäº®
# å¼€å‘æ—¶é—´ ï¼š 2022/7/27 8:44

# 1.1çš„ç›®æ ‡
# -æ‰©å±•æˆ‘ä»¬çš„å›å½’æ¨¡å‹ä¾‹ç¨‹ä»¥æ”¯æŒå¤šç§åŠŸèƒ½
# â€”æ‰©å±•æ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šç§ç‰¹æ€§
# é‡å†™é¢„æµ‹ï¼Œæˆæœ¬å’Œæ¢¯åº¦ä¾‹ç¨‹ï¼Œä»¥æ”¯æŒå¤šç§åŠŸèƒ½
# -åˆ©ç”¨NumPy ' npã€‚ç‚¹æ¥å¯¹å®ƒä»¬çš„å®ç°è¿›è¡ŒçŸ¢é‡åŒ–ï¼Œä»¥è·å¾—é€Ÿåº¦å’Œç®€å•æ€§

# 1.2å·¥å…·
# åœ¨æœ¬å®éªŒå®¤ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨:

# NumPyï¼Œä¸€ä¸ªç”¨äºç§‘å­¦è®¡ç®—çš„æµè¡Œåº“
# Matplotlibï¼Œä¸€ä¸ªå¾ˆæµè¡Œçš„ç»˜åˆ¶æ•°æ®çš„åº“

import copy,math
import numpy as np
import  matplotlib.pyplot as plt

plt.style.use('./deeplearning.mplstyle')
np.set_printoptions(precision=2)  #æ§åˆ¶è¾“å‡ºç»“æœçš„ç²¾åº¦

# ä½ å°†ä½¿ç”¨æˆ¿ä»·é¢„æµ‹çš„æ¿€åŠ±ä¾‹å­ã€‚è®­ç»ƒæ•°æ®é›†åŒ…å«ä¸‰ä¸ªç¤ºä¾‹ï¼Œå…¶ä¸­å››ä¸ªç‰¹å¾(å¤§å°ã€å§å®¤ã€æ¥¼å±‚å’Œå¹´é¾„)å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚
# è¯·æ³¨æ„ï¼Œä¸æ—©æœŸå®éªŒå®¤ä¸åŒçš„æ˜¯ï¼Œå°ºå¯¸æ˜¯ä»¥å¹³æ–¹è‹±å°ºè€Œä¸æ˜¯1000å¹³æ–¹è‹±å°ºä¸ºå•ä½çš„ã€‚
# è¿™å°†å¯¼è‡´ä¸€ä¸ªé—®é¢˜ï¼Œä½ å°†åœ¨ä¸‹ä¸€ä¸ªå®éªŒå®¤è§£å†³!
# æ‚¨å°†ä½¿ç”¨è¿™äº›å€¼å»ºç«‹ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥é¢„æµ‹å…¶ä»–æˆ¿å­çš„ä»·æ ¼ã€‚
# ä¾‹å¦‚ï¼Œæœ‰1200å¹³æ–¹è‹±å°ºï¼Œ3ä¸ªå§å®¤ï¼Œ1å±‚ï¼Œ40å¹´çš„æˆ¿å­ã€‚

# æ‚¨å°†ä½¿ç”¨è¿™äº›å€¼å»ºç«‹ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥é¢„æµ‹å…¶ä»–æˆ¿å­çš„ä»·æ ¼ã€‚
# ä¾‹å¦‚ï¼Œæœ‰1200å¹³æ–¹è‹±å°ºï¼Œ3ä¸ªå§å®¤ï¼Œ1å±‚ï¼Œ40å¹´çš„æˆ¿å­ã€‚
#
# è¯·è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒæ ¼æ¥åˆ›å»ºX_trainå’Œy_trainå˜é‡ã€‚

x_train = np.array([[2104,5,1,45],[1416,3,2,40],[852,2,1,35]])
y_train = np.array([460,232,178])

# print('x shape: {}, x type:{}'.format(x_train.shape,type(x_train)))
# # print(f'x shape: {x_train.shape}, x type:{type(x_train)}')
# print(x_train)
# print('y shape: {}, y type:{}'.format(y_train.shape,type(y_train)))
# print(y_train)

# ğ°æ˜¯ä¸€ä¸ªåŒ…å«ğ‘›ä¸ªå…ƒç´ çš„å‘é‡ã€‚
# æ¯ä¸ªå…ƒç´ åŒ…å«ä¸ä¸€ä¸ªç‰¹æ€§ç›¸å…³è”çš„å‚æ•°ã€‚
# åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ï¼Œnæ˜¯4ã€‚
# ç†è®ºä¸Šï¼Œæˆ‘ä»¬æŠŠå®ƒç”»æˆä¸€ä¸ªåˆ—å‘é‡

b_init = 785.1811367994083
w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])
# print(f"w_init shape: {w_init.shape}, b_init type: {type(b_init)}")

#ç‚¹ç§¯å‡½æ•°
def predict_single_loop(x,w,b):
    '''

    :param x:
    :param w:
    :param b:
    :return:
    '''
    n = x.shape[0]
    p = 0
    for i in range(n):
        p_i = x[i] * w[i]
        p = p + p_i
    p = p + b
    return p

# x__vec = x_train[0,:] #è·å–x_trainçš„ç¬¬é›¶è¡Œ
# print('x__vec shape {}, x__vec value{}:'.format(x__vec.shape,x__vec))

# f_wb = predict_single_loop(x__vec,w_init,b_init)
# print('f_wb shape {}, prediction: {}'.format(f_wb.shape,f_wb))

#ç®€å•å¼ç‚¹ç§¯å‡½æ•°
def predict(x,w,b):
    '''

    :param x:
    :param w:
    :param b:
    :return:
    '''

    p = np.dot(x,w) + b
    return p

x_vec = x_train[0,:]
# print('x_vec shape {}, x_vec value:{}'.format(x_vec.shape,x_vec))
f_wb = predict(x_vec,w_init,b_init)
# print('f_wb shape {}, prediction: {}'.format(f_wb.shape,f_wb))


def compute_cost(x,y,w,b):
    '''
    compute cost
    :param x:
    :param y:
    :param w:
    :param b:
    :return:
    '''
    m = x.shape[0]
    cost = 0.0
    for i in range(m):
        f_wb_i = np.dot(x[i],w) + b
        cost = cost + (f_wb_i - y[i]) ** 2
    cost = cost / (2 * m)
    return cost

cost = compute_cost(x_train,y_train,w_init,b_init)
# print('Cost at optional w: {}'.format(cost))


def compute_gradient(x,y,w,b):
    '''
    Computes the gradient for linear regression
    :param x:
    :param y:
    :param w:
    :param b:
    :return:
    '''
    m, n = x.shape  # (number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.
    for i in range(m):
        err = (np.dot(x[i], w) + b) - y[i]
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err * x[i, j]
        dj_db = dj_db + err
    dj_dw = dj_dw / m
    dj_db = dj_db / m
    return dj_db,dj_dw

tmp_dj_db,tmp_dj_dw = compute_gradient(x_train,y_train,w_init,b_init)
# print('dj_db at initial w,b:{}'.format(tmp_dj_db))
# print('dj_dw at initial w,b: \n{}'.format(tmp_dj_dw))

def gradient_descent(x,y,w_in,b_in,cost_function,gradient_function,alpha,num_iters):
    '''
    æ‰§è¡Œæ‰¹é‡æ¢¯åº¦ä¸‹é™æ¥å­¦ä¹ é€šè¿‡å–æ›´æ–°ã€‚
    å­¦ä¹ é€Ÿç‡ä¸ºalphaçš„num_iteræ¢¯åº¦æ­¥é•¿
    :param x: Data, m examples with n features
    :param y: target values
    :param w_in: initial model parameters
    :param b_in: initial model parameter
    :param cost_function: function to compute cost
    :param gradient_function: function to compute the gradient
    :param alpha: Learning rate
    :param num_iters: è¿è¡Œæ¢¯åº¦ä¸‹é™çš„è¿­ä»£æ¬¡æ•°
    :return:w,b
    '''

    # ä¸€ä¸ªæ•°ç»„ç”¨äºå­˜å‚¨æ¯æ¬¡è¿­ä»£æ—¶çš„å¼€é”€Jå’Œwï¼Œä¸»è¦ç”¨äºä»¥åçš„ç»˜å›¾
    J_history = []
    w = copy.deepcopy(w_in)
    b = b_in

    for i in range(num_iters):
        # Calculate the gradient and update the parameters
        dj_db,dj_dw = gradient_function(x,y,w,b)

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        if i < 100000: #é˜²æ­¢èµ„æºè€—å°½
            J_history.append(cost_function(x,y,w,b))

        # æ¯éš”100æ¬¡æ‰“å°æˆæœ¬
        if i % math.ceil(num_iters / 10) == 0:
            #math.ceil å°æ•°å‘ä¸Šå–æ•´
            #math.floor()  â€œå‘ä¸‹å–æ•´â€ ï¼Œå³å°æ•°éƒ¨åˆ†ç›´æ¥èˆå»
            #math.round()  â€œå››èˆäº”å…¥â€ï¼Œ è¯¥å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªå››èˆäº”å…¥åçš„çš„æ•´æ•°
            print(f'Iteration {i:4d} : Cost {J_history[-1]:8.2f}   ')

    return w,b,J_history

initial_w = np.zeros_like(w_init) #.zeros_like()æ˜¯æŒ‡å’Œæ‹¬å·é‡Œé¢çš„æ ¼å¼ä¸€ä¸‹ä½†æ˜¯æ•°å€¼å…¨ä¸º0
initial_b = 0.

iterations = 1000

alpha = 5.0e-7

w_final,b_final,J_hist =  gradient_descent(x_train,y_train,initial_w,initial_b,
                                         compute_cost,compute_gradient,alpha,iterations)


print(f'b,w found by gradient descent: {b_final:0.2f},{w_final}')
m,_ = x_train.shape
for i in range(m):
    print(f'prediction: {np.dot(x_train[i],w_final) + b_final:0.2f}, target value:{y_train[i]}')

# plot cost versus iteration
fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))
ax1.plot(J_hist)
ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])
ax1.set_title("Cost vs. iteration");  ax2.set_title("Cost vs. iteration (tail)")
ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost')
ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step')
plt.show()