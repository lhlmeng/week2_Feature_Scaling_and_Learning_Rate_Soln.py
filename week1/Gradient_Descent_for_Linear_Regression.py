# å·¥ä½œå•ä½ ï¼š æ­å·ç”µå­ç§‘æŠ€å¤§å­¦
# å§“å ï¼š ç½—ä¼šäº®
# å¼€å‘æ—¶é—´ ï¼š 2022/7/25 8:10

import math,copy
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
from lab_utils_uni import plt_house_x,plt_contour_wgrad,plt_divergence,plt_gradients

x_train = np.array([1.0,2.0])
y_train = np.array([300.0,500.0])

# # ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))=ğ‘¤ğ‘¥(ğ‘–)+ğ‘                                  å…¬å¼(1)
# ğ½(ğ‘¤,ğ‘)=12ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))**2                 å…¬å¼(2)
# w = w - alpha * dj_dw
# b = b - alpha * dj_db                                å…¬å¼(3)
#
# dj_dw = (1 / m) * âˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–)) * ğ‘¥(ğ‘–)      å…¬å¼(4)
# dj_db = (1 / m) * âˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))             å…¬å¼(5)


#Function to calculate the cost
def compute_cost(x,y,w,b):
    m = x.shape[0]
    cost = 0

    for i in range(m):
        f_wb = w * x[i] + b
        cost = cost + (f_wb * x[i] - y[i]) ** 2
        pass
    total_cost =1 / (2 * m) * cost

    return total_cost

def compute_gradient(x,y,w,b):
    '''
    Compute the gradient for linear regression
    :param x:
    :param y:
    :param w:
    :param b:
    :return:
    dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
    dj_db (scalar): The gradient of the cost w.r.t. the parameter b
    '''

    #Number of training examples
    m = x.shape[0]
    dj_dw = 0
    dj_db = 0

    for i in range(m):
        f_wb = w * x[i] + b
        dj_dw_i = (f_wb - y[i]) * x[i]
        dj_db_i = f_wb - y[i]
        dj_dw += dj_dw_i
        dj_db += dj_db_i
    dj_dw = dj_dw / m
    dj_db = dj_db / m
    return dj_dw,dj_db

plt_gradients(x_train,y_train,compute_cost,compute_gradient)
plt.show()

def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):
    '''
    å¯¹w,bè¿›è¡Œæ¢¯åº¦ä¸‹é™æ‹Ÿåˆã€‚é€šè¿‡è·å–æ›´æ–°w,bã€‚
    å­¦ä¹ é€Ÿç‡ä¸ºalphaçš„num_iteræ¢¯åº¦æ­¥é•¿
    :param x: m examples
    :param y: target values
    :param w_in:æ¨¡å‹å‚æ•°çš„åˆå§‹å€¼
    :param b_in:æ¨¡å‹å‚æ•°çš„åˆå§‹å€¼
    :param alpha:å­¦ä¹ ç‡
    :param num_iters:è¿è¡Œæ¢¯åº¦ä¸‹é™çš„è¿­ä»£æ¬¡æ•°
    :param cost_function:å‡½æ•°è°ƒç”¨äº§ç”Ÿæˆæœ¬
    :param gradient_function:å‡½æ•°æ¥è°ƒç”¨ä»¥äº§ç”Ÿæ¢¯åº¦
    :return:
    w (scalar): è¿è¡Œæ¢¯åº¦ä¸‹é™åæ›´æ–°å‚æ•°å€¼
    b (scalar): è¿è¡Œæ¢¯åº¦ä¸‹é™åæ›´æ–°å‚æ•°å€¼
    J_history (List): å†å²æˆæœ¬å€¼
    p_history (list): å‚æ•°[w,b]çš„å†å²
    '''

    #é˜²æ­¢ä¿®æ”¹å…¨å±€å˜é‡w_in
    w = copy.deepcopy(w_in)
    #copy.deepcopy()å‡½æ•°æ˜¯ä¸€ä¸ªæ·±å¤åˆ¶å‡½æ•°ã€‚
    # æ‰€è°“æ·±å¤åˆ¶ï¼Œå°±æ˜¯ä»è¾“å…¥å˜é‡å®Œå…¨å¤åˆ»ä¸€ä¸ªç›¸åŒçš„å˜é‡ï¼Œæ— è®ºæ€ä¹ˆæ”¹å˜æ–°å˜é‡ï¼ŒåŸæœ‰å˜é‡çš„å€¼éƒ½ä¸ä¼šå—åˆ°å½±å“ã€‚

    #
    #åœ¨æ¯æ¬¡è¿­ä»£æ—¶å­˜å‚¨æˆæœ¬Jå’Œwçš„æ•°ç»„ï¼Œä¸»è¦ç”¨äºä»¥åçš„ç»˜å›¾
    J_history = []
    p_history = []
    b = b_in
    w = w_in

    for i in range(num_iters):
        # è®¡ç®—æ¢¯åº¦å¹¶ä½¿ç”¨gradient_functionæ›´æ–°å‚æ•°
        dj_dw, dj_db = gradient_function(x,y,w,b)

        #ä½¿ç”¨å…¬å¼(3)æ›´æ–°å‚æ•°
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

    # ä¿å­˜æ¯æ¬¡è¿­ä»£æˆæœ¬J
    if i < 100000:  #é˜²æ­¢èµ„æºè€—å°½
        J_history.append(cost_function(x,y,w,b))
        p_history.append([w, b])

    # å¦‚æœ < 10ï¼Œåˆ™æ¯éš”10æ¬¡æˆ–ç›¸åŒæ¬¡æ•°æ‰“å°æˆæœ¬
    if i % math.ceil(num_iters / 10) == 0:
        print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
              f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
              f"w: {w: 0.3e}, b:{b: 0.5e}")

    return w,b,J_history,p_history

# initialize parameters
w_init = 0
b_init = 0
# some gradient descent settings
iterations = 10000
tmp_alpha = 1.0e-2
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,
                                                    iterations, compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})")

# plot cost versus iteration
fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))
ax1.plot(J_hist[:100])
ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])
ax1.set_title("Cost vs. iteration(start)");  ax2.set_title("Cost vs. iteration (end)")
ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')
ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')
plt.show()

print(f"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars")
print(f"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars")
print(f"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars")


fig, ax = plt.subplots(1,1, figsize=(12, 6))
plt_contour_wgrad(x_train, y_train, p_hist, ax)


fig, ax = plt.subplots(1,1, figsize=(12, 4))
plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],
            contours=[1,5,10,20],resolution=0.5)


# initialize parameters
w_init = 0
b_init = 0
# set alpha to a large value
iterations = 10
tmp_alpha = 8.0e-1
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,
                                                    iterations, compute_cost, compute_gradient)

# plt_divergence(p_hist, J_hist,x_train, y_train)
plt.show()